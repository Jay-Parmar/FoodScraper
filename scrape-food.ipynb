{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7241d732",
   "metadata": {},
   "source": [
    "### Install all this before running the code\n",
    "\n",
    "pip install selenium beautifulsoup4 ndjson requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "19bf637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import gzip\n",
    "import ndjson\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "07d987e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrabFoodScraper:\n",
    "    \"\"\"\n",
    "    This class allows you to fetch and parse data from the given website and\n",
    "    get the details of the restaurants in a compressed file format.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, location_url):\n",
    "        self.location_url = location_url\n",
    "        self.restaurants = []\n",
    "        self.base_url = \"https://food.grab.com\"\n",
    "        self.init_webdriver()\n",
    "    \n",
    "    def init_webdriver(self):\n",
    "        \"\"\"\n",
    "        Initialize Selenium WebDriver with Chrome options to mimic a regular browser session and avoid detection.\n",
    "        \"\"\"\n",
    "        ### Troubleshooting to prevent cloudfront to block out IP ###\n",
    "        \n",
    "        # proxy = \"socks5://127.0.0.1:9050\"\n",
    "        # firefox_options = webdriver.FirefoxOptions()\n",
    "        # profile = webdriver.FirefoxOptions()\n",
    "        # driver = webdriver.Firefox(options=options)\n",
    "\n",
    "        # firefox_options.add_argument(f\"User-Agent={user_agent}\")\n",
    "        # profile.set_preference(\"general.useragent.override\", user_agent)\n",
    "        # firefox_options.add_argument(f\"user-agent={user_agent}\")\n",
    "        # chrome_options.add_argument('--proxy-server=%s' % proxy)\n",
    "        # chrome_options.add_argument(\"--headless\")\n",
    "        # chrome_options.add_argument(\"--disable-gpu\")\n",
    "        # self.driver = webdriver.Firefox(profile)\n",
    "        \n",
    "        chrome_options = Options()\n",
    "        user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.e.4472.124 Safari/537.36\"\n",
    "        chrome_options.add_argument(f\"user-agent={user_agent}\")\n",
    "        self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "    \n",
    "    def navigate_to_location(self):\n",
    "        \"\"\"\n",
    "        Open the location URL in a WebDriver session.\n",
    "        \"\"\"\n",
    "        self.driver.get(self.location_url)\n",
    "        # Allow time for page to load\n",
    "        time.sleep(5)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def go_to_address(self, address):\n",
    "        \"\"\"\n",
    "        Navigate to a specific address within the site to fetch restaurant data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # We have to waith until the location search bar is visible\n",
    "            self.wait.until(\n",
    "                EC.presence_of_element_located((By.ID, \"location-input\"))\n",
    "            )\n",
    "            input_address = self.driver.find_element('id', 'location-input')\n",
    "            input_address.send_keys(address)\n",
    "            # Troubleshooting to prevent cloudfront to block out IP\n",
    "            time.sleep(5)\n",
    "            search_button = self.driver.find_element(By.CLASS_NAME, 'submitBtn___2roqB')\n",
    "            search_button.click()\n",
    "        except TimeoutException:\n",
    "            print(\"Timed out waiting for page to load\")\n",
    "            \n",
    "                \n",
    "    def reach_end_of_page(self):\n",
    "        \"\"\"\n",
    "        Scroll to the bottom of the page dynamically to load all restaurant listings.\n",
    "        \"\"\"\n",
    "        last_height = self.driver.execute_script('return document.body.scrollHeight')\n",
    "        \n",
    "        while True:\n",
    "            self.driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "            # Wait for the page to load new content\n",
    "            time.sleep(3)\n",
    "            \n",
    "            new_height = self.driver.execute_script('return document.body.scrollHeight')\n",
    "            \n",
    "            if new_height == last_height:\n",
    "                # End of page reached\n",
    "                break\n",
    "            last_height = new_height\n",
    "    \n",
    "    \n",
    "    def extract_restaurant_data(self):\n",
    "        \"\"\"\n",
    "        Extract raw HTML data from the page for parsing.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            page_source = self.driver.page_source\n",
    "            self.soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            print(\"Yay! Soup created!\")\n",
    "        except TimeoutException:\n",
    "            print(\"Timed out waiting for page to load\")\n",
    "        finally:\n",
    "            # To quit the current webdriver session\n",
    "            self.driver.quit()\n",
    "            \n",
    "    def parse_restaurant_data(self):\n",
    "        \"\"\"\n",
    "        Parse the restaurant data from HTML and store it in structured format.\n",
    "        \"\"\"\n",
    "        restaurants = self.soup.find_all(class_='RestaurantListCol___1FZ8V')\n",
    "        for restaurant in restaurants:\n",
    "            name = restaurant.find(class_='name___2epcT').text\n",
    "            cuisine = restaurant.find(class_='cuisine___T2tCh').text\n",
    "            num_info = restaurant.find_all(class_='numbersChild___2qKMV')\n",
    "            time_taken, dist = parse_time_distance(num_info[-1].text)\n",
    "            rating = None if len(num_info)==1 else num_info[0].text\n",
    "            discount_text = [discount.text for discount in restaurant.find_all(class_='discountText___GQCkj')]\n",
    "\n",
    "            # Can't find any notice on any restaurants\n",
    "            # notice = restaurant.find(class_='TBD')\n",
    "            image_link = restaurant.find(class_='realImage___2TyNE').get('src')\n",
    "            has_promo = len(restaurant.find_all(class_='promoTagHead___1bjRG'))>0\n",
    "            restaurant_href = restaurant.find('a')['href']\n",
    "            restaurant_id = restaurant_href.split('/')[-1].strip(' ?')\n",
    "\n",
    "            data = {\n",
    "                'Id': restaurant_id,\n",
    "                'Name': name,\n",
    "                'Cuisine': cuisine,\n",
    "                'Time Taken': time_taken,\n",
    "                'Distance': dist,\n",
    "                'Restaurant Rating': rating,\n",
    "                'Promo Text': discount_text[0] if discount_text else None,\n",
    "                'Image Link': image_link,\n",
    "                'Has Promo': has_promo,\n",
    "            }\n",
    "            # print(data)\n",
    "            self.restaurants.append(data)\n",
    "\n",
    "    \n",
    "    def get_lat_lon(self, restaurant_id):\n",
    "        # Longitude and Latitude aren't on the detail pages as far as I know. If we can find those on\n",
    "        # detail page, we can make a soup of those page like this (below code) and fetch the details similarly.\n",
    "\n",
    "        # detail_page = requests.get(self.base_url + restaurant_href)\n",
    "        # grab_soup = BeautifulSoup(detail_page.text, 'html.parser')\n",
    "        \n",
    "        # Placeholder method to fetch latitude and longitude using restaurant_id\n",
    "        return \"1.3521\", \"103.8198\"\n",
    "    \n",
    "\n",
    "    def save_to_ndjson(self, filename):\n",
    "        \"\"\"\n",
    "        Save extracted data to NDJSON format.\n",
    "        \"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            writer = ndjson.writer(f, ensure_ascii=False)\n",
    "            for restaurant in self.restaurants:\n",
    "                writer.writerow(json.dumps(restaurant))\n",
    "\n",
    "    def compress_file(self, filename):\n",
    "        \"\"\"\n",
    "        Compress the NDJSON file into GZip format.\n",
    "        \"\"\"\n",
    "        with open(filename, 'rb') as f_in:\n",
    "            with gzip.open(filename + '.gz', 'wb') as f_out:\n",
    "                f_out.writelines(f_in)\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Execute the scraper: setting up, navigating, data extraction, parsing, saving, and compressing.\n",
    "        \"\"\"\n",
    "        address1 = \"Chong Boon Dental Surgery - Block 456 Ang Mo Kio Avenue 10, #01-1574, Singapore, 560456\"\n",
    "        address2 = \"PT Singapore - Choa Chu Kang North 6, Singapore, 689577\"\n",
    "        \n",
    "        # Use threading to handle different parts of the task concurrently\n",
    "        threads = []\n",
    "        t1 = threading.Thread(target=self.navigate_to_location)\n",
    "        threads.append(t1)\n",
    "        t2 = threading.Thread(target=self.go_to_address, args=(address,))\n",
    "        threads.append(t2)\n",
    "        \n",
    "        # Starting navigation and input threads\n",
    "        for t in threads:\n",
    "            t.start()\n",
    "        \n",
    "        # Wait for navigation and input to complete\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "            \n",
    "        # self.navigate_to_location()\n",
    "        # self.go_to_address(address2)\n",
    "        time.sleep(5)\n",
    "        self.reach_end_of_page()\n",
    "        self.extract_restaurant_data()\n",
    "        self.parse_restaurant_data()\n",
    "        self.save_to_ndjson(\"restaurants.ndjson\")\n",
    "        self.compress_file(\"restaurants.ndjson\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2dcace91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yay! Soup created!\n"
     ]
    }
   ],
   "source": [
    "scraper = GrabFoodScraper(\"https://food.grab.com/sg/en/\")\n",
    "scraper.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c0cb7",
   "metadata": {},
   "source": [
    "#### How many restaurents are fetched - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "71f8fef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(scraper.restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b6f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fd42f36",
   "metadata": {},
   "source": [
    "#### Debug code acting like a playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "efc83792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Id': '4-C6L3ERDYFACEGE', 'Name': 'Fiesta Mexico - Three Amigos Indian and Mexican Restaurant', 'Cuisine': 'Mexican, Western, Indian', 'Time Taken': '25 mins', 'Distance': '1.3 km', 'Restaurant Rating': None, 'Promo Text': None, 'Image Link': 'https://food-cms.grab.com/compressed_webp/merchants/4-C6L3ERDYFACEGE/hero/95d909e4cd014c8e95cb9fa951db6da5_1715937184937605107.webp', 'Has Promo': False}\n",
      "{'Id': 'SGDD04944', 'Name': \"McDonald's - Boat Quay\", 'Cuisine': 'Burger, Fast Food, Halal', 'Time Taken': '20 mins', 'Distance': '1.5 km', 'Restaurant Rating': '4.2', 'Promo Text': 'S$0.20 off on selected items', 'Image Link': 'https://food-cms.grab.com/compressed_webp/merchants/SGDD04944/hero/d2447ac38a2541769b115b2a1d25291d_1677816675499200090.webp', 'Has Promo': True}\n",
      "{'Id': '4-C6K3LLJBMFKZCT', 'Name': 'Burger King - Boat Quay', 'Cuisine': 'Burgers, Fast Food', 'Time Taken': '25 mins', 'Distance': '1.3 km', 'Restaurant Rating': '4.9', 'Promo Text': None, 'Image Link': 'https://food-cms.grab.com/compressed_webp/merchants/4-C6K3LLJBMFKZCT/hero/c1cd7fe27ff742d5a4bf4dd216b9a3a2_1714530917680052066.webp', 'Has Promo': False}\n",
      "{'Id': '4-C6LVLKKTL6LBV2', 'Name': 'Maharaja Palace - Boat Quay', 'Cuisine': 'Indian, Seafood, Drinks & Beverages', 'Time Taken': '35 mins', 'Distance': '1.3 km', 'Restaurant Rating': '1', 'Promo Text': None, 'Image Link': 'https://food-cms.grab.com/compressed_webp/merchants/4-C6LVLKKTL6LBV2/hero/739b2f9e032b4866a743f93bce58c1ae_1715316417201384856.webp', 'Has Promo': False}\n",
      "[{'Id': '4-C6L3ERDYFACEGE', 'Name': 'Fiesta Mexico - Three Amigos Indian and Mexican Restaurant', 'Cuisine': 'Mexican, Western, Indian', 'Time Taken': '25 mins', 'Distance': '1.3 km', 'Restaurant Rating': None, 'Promo Text': None, 'Image Link': 'https://food-cms.grab.com/compressed_webp/merchants/4-C6L3ERDYFACEGE/hero/95d909e4cd014c8e95cb9fa951db6da5_1715937184937605107.webp', 'Has Promo': False}, {'Id': 'SGDD04944', 'Name': \"McDonald's - Boat Quay\", 'Cuisine': 'Burger, Fast Food, Halal', 'Time Taken': '20 mins', 'Distance': '1.5 km', 'Restaurant Rating': '4.2', 'Promo Text': 'S$0.20 off on selected items', 'Image Link': 'https://food-cms.grab.com/compressed_webp/merchants/SGDD04944/hero/d2447ac38a2541769b115b2a1d25291d_1677816675499200090.webp', 'Has Promo': True}, {'Id': '4-C6K3LLJBMFKZCT', 'Name': 'Burger King - Boat Quay', 'Cuisine': 'Burgers, Fast Food', 'Time Taken': '25 mins', 'Distance': '1.3 km', 'Restaurant Rating': '4.9', 'Promo Text': None, 'Image Link': 'https://food-cms.grab.com/compressed_webp/merchants/4-C6K3LLJBMFKZCT/hero/c1cd7fe27ff742d5a4bf4dd216b9a3a2_1714530917680052066.webp', 'Has Promo': False}, {'Id': '4-C6LVLKKTL6LBV2', 'Name': 'Maharaja Palace - Boat Quay', 'Cuisine': 'Indian, Seafood, Drinks & Beverages', 'Time Taken': '35 mins', 'Distance': '1.3 km', 'Restaurant Rating': '1', 'Promo Text': None, 'Image Link': 'https://food-cms.grab.com/compressed_webp/merchants/4-C6LVLKKTL6LBV2/hero/739b2f9e032b4866a743f93bce58c1ae_1715316417201384856.webp', 'Has Promo': False}]\n"
     ]
    }
   ],
   "source": [
    "def parse_time_distance(string):\n",
    "    string = string.replace('\\xa0', ' ')\n",
    "    parts = string.split('•')\n",
    "    if len(parts) == 2:\n",
    "        time_part = parts[0].strip()\n",
    "        distance_part = parts[1].strip()\n",
    "        return time_part, distance_part\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "base_url = \"https://food.grab.com\"\n",
    "yo = []\n",
    "    \n",
    "for restaurant in restaurants[:4]:\n",
    "    name = restaurant.find(class_='name___2epcT').text\n",
    "    cuisine = restaurant.find(class_='cuisine___T2tCh').text\n",
    "    num_info = restaurant.find_all(class_='numbersChild___2qKMV')\n",
    "    time_taken, dist = parse_time_distance(num_info[-1].text)\n",
    "    rating = None if len(num_info)==1 else num_info[0].text\n",
    "    discount_text = [discount.text for discount in restaurant.find_all(class_='discountText___GQCkj')]\n",
    "    \n",
    "    # Can't find any notice on any restaurants\n",
    "    # notice = restaurant.find(class_='TBD')\n",
    "    image_link = restaurant.find(class_='realImage___2TyNE')['src']\n",
    "    has_promo = len(restaurant.find_all(class_='promoTagHead___1bjRG'))>0\n",
    "    restaurant_href = restaurant.find('a')['href']\n",
    "    restaurant_id = restaurant_href.split('/')[-1].strip(' ?')\n",
    "    \n",
    "    \n",
    "    # Longitude and Latitude aren't on the detail pages as far as I know. If we can find those on\n",
    "    # detail page, we can make a soup of those page like this (below code) and fetch the details similarly.\n",
    "    \n",
    "    # detail_page = requests.get(base_url + restaurant_href)\n",
    "    # grab_soup = BeautifulSoup(detail_page.text, 'html.parser')\n",
    "    \n",
    "    \n",
    "    data = {\n",
    "        'Id': restaurant_id,\n",
    "        'Name': name,\n",
    "        'Cuisine': cuisine,\n",
    "        'Time Taken': time_taken,\n",
    "        'Distance': dist,\n",
    "        'Restaurant Rating': rating,\n",
    "        'Promo Text': discount_text[0] if discount_text else None,\n",
    "        'Image Link': image_link,\n",
    "        'Has Promo': has_promo,\n",
    "    }\n",
    "    print(data)\n",
    "    yo.append(data)\n",
    "\n",
    "\n",
    "# headers = ['Id', 'Name', 'Cuisine', 'Delivery Time', 'Distance', 'Rating', 'Promo Text', 'Image Link', 'Has Promo']\n",
    "print(yo)\n",
    "restaurant_df = pd.DataFrame(yo)\n",
    "restaurant_df.to_csv('lets_see.csv')\n",
    "ndjson_str = '\\n'.join(json.dumps(record) for record in yo) + '\\n'\n",
    "\n",
    "# Compress the ndjson string\n",
    "with gzip.open('restaurants.ndjson.gz', 'wt', encoding='utf-8') as f:\n",
    "    f.write(ndjson_str)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66b43a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
